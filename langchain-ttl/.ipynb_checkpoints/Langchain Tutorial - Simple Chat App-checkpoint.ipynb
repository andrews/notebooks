{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5bb199",
   "metadata": {},
   "source": [
    "# Langchain Tutorial\n",
    "Oh boy, remember how annoying it was to get it set up on windows... Let's see if it works here. Following [this](https://python.langchain.com/docs/tutorials/llm_chain/) tutorial!\n",
    "\n",
    "## Trying to use venv...\n",
    "First needed to create a virtual environment. Created the virtual environment `lc-ttl` somewhere else, activated it, installed ipykernel using\n",
    "\n",
    "```pip install ipykernel```\n",
    "\n",
    "then ran\n",
    "\n",
    "```python -m ipykernel install --user --name=lc-ttl```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc09935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# langchain\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2e9f2",
   "metadata": {},
   "source": [
    "At this point, verified which model I'm going to use (`gemini-1.5-flash`) for this exercise. Sort of verified where it exists in GCP... Apparently there's a difference between the Vertex AI API and Generative Language API.\n",
    "\n",
    "Also created and exported the Langsmith API key locally, so as to not input it every time here. \n",
    "\n",
    "Actually this didn't really work lol. Just used above. The traces now show in Langsmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this actually worked now wtf\n",
    "!gcloud info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848cdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when did this get installed? maybe when I installed the vertex langchain package\n",
    "# seems to only work in the lc-ttl kernel, so the above is most likely the case\n",
    "import google.auth\n",
    "\n",
    "_, project_id = google.auth.default()\n",
    "# print(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ba3f2",
   "metadata": {},
   "source": [
    "After much frustration, the above and below lines of code actually worked. Had an easier time this time, already knew I needed to install gcloud and set a default project. However, the above and below still weren't working. Tried these two things\n",
    "  * Setting the default project again in gcloud and then *re*-authenticating. This resulted in google.auth.default() showing the expected output in my terminal. However, this still wasn't working in my jupyter notebook.\n",
    "  * Closing the jupyter server and then starting it from a new terminal window. This made everything work. Resetting the kernel usually refreshes everything, and you usually need to use a new terminal window after installing things. I did the first thing often but not the second. Selecting the right kernel/virtualenv is also important.\n",
    "  \n",
    "Shout out to chatGPT for these two suggestions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00899fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-1.5-flash\", model_provider=\"google_vertexai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04207dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ehi, dove √® la mia pizza? \\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.046630859375, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.061767578125}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.12109375, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.06640625}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.09521484375, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.0390625}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.12109375, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.111328125}], 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 11, 'total_token_count': 26, 'prompt_tokens_details': [{'modality': 1, 'token_count': 15}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 11}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.0400840856812217}, id='run-d369cbae-fbed-46d3-a1e4-cde2c7612c5d-0', usage_metadata={'input_tokens': 15, 'output_tokens': 11, 'total_tokens': 26})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat models are instances of runnables\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"Hey, where's my pizza?\")\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9079ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! üëã  How can I help you today? üòä \\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.032470703125, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.036865234375}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.056640625, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.046142578125}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.055908203125, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.0185546875}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.1484375, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.0267333984375}], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 14, 'total_token_count': 15, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 14}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.045433333941868374}, id='run-5efbbff9-f51c-47d7-b6e9-e573631af0c1-0', usage_metadata={'input_tokens': 1, 'output_tokens': 14, 'total_tokens': 15})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different ways to invoke the model\n",
    "model.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c92411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! üëã  How can I help you today? üòä \\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.032470703125, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.036865234375}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.056640625, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.046142578125}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.055908203125, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.0185546875}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.1484375, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.0267333984375}], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 14, 'total_token_count': 15, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 14}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.045433333941868374}, id='run-49e341ea-0af5-486c-9d12-7c6caae989d2-0', usage_metadata={'input_tokens': 1, 'output_tokens': 14, 'total_tokens': 15})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6992fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! üëã  How can I help you today? üòä \\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.032470703125, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.036865234375}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.056640625, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.046142578125}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.055908203125, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.0185546875}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'probability_score': 0.1484375, 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE', 'severity_score': 0.0267333984375}], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 14, 'total_token_count': 15, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 14}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.045433333941868374}, id='run-557aff81-7a64-4dd3-b673-9e085c23aa9f-0', usage_metadata={'input_tokens': 1, 'output_tokens': 14, 'total_tokens': 15})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(\"Hello\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430acaa",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "Because chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5210c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E|hi, dov'√® la mia pizza? \n",
      "|"
     ]
    }
   ],
   "source": [
    "# streaming\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1379bc",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "Up until now we've been invoking the model directly with hardcoded messages. We may want to apply some transformations to raw input before passing it to the model. Can include system messages or formatting a template with user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23688272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "# contains a system message and a user message, like we had before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd97c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into korean', additional_kwargs={}, response_metadata={}), HumanMessage(content='I really want to play league of legends', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"korean\", \"text\": \"I really want to play league of legends\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4760fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into korean', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I really want to play league of legends', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the messages directly\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "232f515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ÄÎäî Ï†ïÎßê Î¶¨Í∑∏ Ïò§Î∏å Î†àÏ†ÑÎìúÎ•º ÌïòÍ≥† Ïã∂Ïñ¥Ïöî. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e07280",
   "metadata": {},
   "source": [
    "Seems like Langsmith recorded the streaming part, when we created the prompt templates, and when we invoked the model!\n",
    "\n",
    "The tutorial was a success. I'm not completely sure which Google API was used though... I only have a key/credential enabled for Generative Language API. The Langchain tutorial used a package for Vertex AI. Costs and API usage may take some time to show up in GCP. Check later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Langchain)",
   "language": "python",
   "name": "lc-ttl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
